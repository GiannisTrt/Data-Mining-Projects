{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by Giannis Taratsas(sdi1700160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are gonna import the libraries that we will use to our queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os,sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords as stwords\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from numpy import interp\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import KFold,StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are gonna insert the csvs. The train_data set is to train our data. The test_data set has the comments that we will try to find the results. sol_test_data has the same data as the test_data set but it also includes one column with the solutuions with which we are gonna test our predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Location=r'C:\\Tede\\Apal_ergasia\\data\\train.csv'\n",
    "train_data = pd.read_csv(Location)\n",
    "\n",
    "Location=r'C:\\Tede\\Apal_ergasia\\data\\impermium_verification_labels.csv'\n",
    "sol_test_data = pd.read_csv(Location)\n",
    "\n",
    "Location=r'C:\\Tede\\Apal_ergasia\\data\\impermium_verification_set.csv'\n",
    "test_data = pd.read_csv(Location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a function that removes all the special characters. Firstly, we delete all the words with '\\\\' in them (in this way we delete special characters such as \\xa0. Some words are united with the \\xa0 and the other special characters for example \\xa0bring, We delete them too because they are not a lot and we can handle this kind of missing information. Secondly, we remove the urls and lastly we remove all the other punctuation so we have better results to our classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s): #function to remove special characters\n",
    "    \n",
    "    slash_back = r'\\s*(?:[\\w_]*\\\\(?:[\\w_]*\\\\)*[\\w_]*)'  # this is used to remove the backslashes when we find one we remove all the word\n",
    "    stripped=re.sub(slash_back,\"\",s)\n",
    "    \n",
    "    p=re.compile(r'\\<http.+?\\>', re.DOTALL)    # this is used to remove urls \n",
    "    stripped=re.sub(p, '', stripped)\n",
    "    \n",
    "    stripped= re.sub('[^A-Za-z0-9]+', ' ', stripped) # in this way we remove all the other punctuation in the text\n",
    " \n",
    "    # Change any white space to one space \n",
    "    stripped = re.sub('\\s+', ' ', stripped) \n",
    "    \n",
    "    # Remove start and end white spaces \n",
    "    stripped = stripped.strip() \n",
    "  #  if stripped != '': \n",
    "    return stripped.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to make our word vectors with CountVectorizer. The text preprocessing and cleaning is done with the function that I declared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bow vectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, ngram_range = (1,1))\n",
    "\n",
    "#make train BoW\n",
    "temp = train_data.copy()\n",
    "train=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    train.append(remove_string_special_characters(x))\n",
    "\n",
    "bow_train = bow_vectorizer.fit_transform(train)\n",
    "\n",
    "#make test BoW\n",
    "temp = test_data.copy()\n",
    "test=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    test.append(remove_string_special_characters(x))\n",
    "    \n",
    "bow_test = bow_vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to use our Naive Bayes Classifier to fit and predict the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes bow\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(bow_train.toarray(), train_data['Insult'].array);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results(accuracy score and f1 metric) we get from the basic format of the naive bayes algorithm after only cleaning our data as asked are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.5248322147651007\n",
      "f1 score is: 0.5220522052205221\n"
     ]
    }
   ],
   "source": [
    "y_pred=gnb.predict(bow_test.toarray())\n",
    "print(\"Accuracy score is:\" ,metrics.accuracy_score(sol_test_data['Insult'].array, y_pred))\n",
    "print(\"f1 score is:\" ,metrics.f1_score(sol_test_data['Insult'].array, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are gonna try to make Naive Bayes better using the methods that are asked. Every method is going to used seperately and we are gonna see if this makes our results better or worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this function to unite the words after the lemmatization to a sting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string   \n",
    "    return (str1.join(s)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to make our bow_train and bow_test, we clean our data, then do lemmatization and lastly we make again the same sentences but now the words are lemmatized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bow vectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1)\n",
    "\n",
    "#make train BoW\n",
    "text_train=[]\n",
    "temp = train_data.copy()\n",
    "train=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    train.append(remove_string_special_characters(x))\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "train1 = [[lmtzr.lemmatize(word) for word in word_tokenize(s)]\n",
    "              for s in train]\n",
    "for i in train1:\n",
    "    text_train.append(listToString(i))\n",
    "\n",
    "bow_train = bow_vectorizer.fit_transform(text_train)\n",
    "\n",
    "#make test BoW\n",
    "text_test=[]\n",
    "temp = test_data.copy()\n",
    "test=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    test.append(remove_string_special_characters(x))\n",
    "    \n",
    "\n",
    "test1 = [[lmtzr.lemmatize(word) for word in word_tokenize(s)]\n",
    "              for s in test]\n",
    "for i in test1:\n",
    "    text_test.append(listToString(i))\n",
    "    \n",
    "bow_test = bow_vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to use our Naive Bayes Classifier to fit and predict the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes bow\n",
    "gnb.fit(bow_train.toarray(), train_data['Insult'].array);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we are getting now are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.5279642058165548\n",
      "f1 score is: 0.5325653522374834\n"
     ]
    }
   ],
   "source": [
    "y_pred=gnb.predict(bow_test.toarray())\n",
    "print(\"Accuracy score is:\" ,metrics.accuracy_score(sol_test_data['Insult'].array, y_pred))\n",
    "print(\"f1 score is:\" ,metrics.f1_score(sol_test_data['Insult'].array, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a small improvement both in the accuracy score and the f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly,we make a list with stopwords from wordcloud and nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = set(stwords.words('english')) \n",
    "stop_list.update(set(STOPWORDS))                        #wordcloud stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do exactly the same as above but now after cleaning and preprocessing the  text data and we also, delete the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['let'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#make bow vectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1,ngram_range=(1,1),stop_words=stop_list)\n",
    "\n",
    "#make train BoW\n",
    "temp = train_data.copy()\n",
    "train=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    train.append(remove_string_special_characters(x))\n",
    "\n",
    "bow_train = bow_vectorizer.fit_transform(train)\n",
    "\n",
    "#make test BoW\n",
    "temp = test_data.copy()\n",
    "test=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    test.append(remove_string_special_characters(x))\n",
    "    \n",
    "bow_test = bow_vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes bow\n",
    "gnb.fit(bow_train.toarray(), train_data['Insult'].array);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.5225950782997763\n",
      "f1 score is: 0.5213100044863167\n"
     ]
    }
   ],
   "source": [
    "y_pred=gnb.predict(bow_test.toarray())\n",
    "print(\"Accuracy score is:\" ,metrics.accuracy_score(sol_test_data['Insult'].array, y_pred))\n",
    "print(\"f1 score is:\" ,metrics.f1_score(sol_test_data['Insult'].array, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after deleting the stopwords from the texts we had a slight decrease in our metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing that changes in the bigrams is than now we use ngram_range=(2,2) in Countvectorizer. Bigrams basically create duos of words instead of taking every word separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bow vectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, ngram_range=(2,2))\n",
    "\n",
    "#make train BoW\n",
    "temp = train_data.copy()\n",
    "train=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    train.append(remove_string_special_characters(x))\n",
    "\n",
    "bow_train = bow_vectorizer.fit_transform(train)\n",
    "\n",
    "#make test BoW\n",
    "temp = test_data.copy()\n",
    "test=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    test.append(remove_string_special_characters(x))\n",
    "    \n",
    "bow_test = bow_vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes bow\n",
    "gnb.fit(bow_train.toarray(), train_data['Insult'].array);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the results are the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.5574944071588367\n",
      "f1 score is: 0.5292717753450737\n"
     ]
    }
   ],
   "source": [
    "y_pred=gnb.predict(bow_test.toarray())\n",
    "print(\"Accuracy score is:\" ,metrics.accuracy_score(sol_test_data['Insult'].array, y_pred))\n",
    "print(\"f1 score is:\" ,metrics.f1_score(sol_test_data['Insult'].array, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must note that after using bigrams the accuracy score increased about 3% and the f1 metric increased about 1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)Laplace Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Laplace Smoothing we must make Multinomial Naive Bayes instead of gaussian with alpha=1. That means that we don't need to change nothing to the text data, but only clean them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bow vectorizer\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1)\n",
    "\n",
    "#make train BoW\n",
    "temp = train_data.copy()\n",
    "train=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    train.append(remove_string_special_characters(x))\n",
    "\n",
    "bow_train = bow_vectorizer.fit_transform(train)\n",
    "\n",
    "#make test BoW\n",
    "temp = test_data.copy()\n",
    "test=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    test.append(remove_string_special_characters(x))\n",
    "    \n",
    "bow_test = bow_vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after cleaning all the data the results are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.6711409395973155\n",
      "f1 score is: 0.6193682030036252\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=1.0)\n",
    "clf.fit(bow_train.toarray(), train_data['Insult'].array);\n",
    "y_pred=clf.predict(bow_test.toarray())\n",
    "print(\"Accuracy score is:\" ,metrics.accuracy_score(sol_test_data['Insult'].array, y_pred))\n",
    "print(\"f1 score is:\" ,metrics.f1_score(sol_test_data['Insult'].array, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using all the methods which where mentioned to make our results better, we can easily notice that Laplace Smoothing made the bigger impact! The accuracy score in now **15% bigger** than the first one and the f1 metric **increased about 9%** ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also conclude that Muntinomial Gausian Bayes is a better fit for this dataset than Gaussian, because it gives us better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF AND PART OF SPEECH ARRAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a composite of TF-IDF and Part-Of-Speech Array we firstly need to make the tfidf array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the TFIDF array for the train and the test files are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['let'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#make tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, ngram_range=(1,2), max_features=3000,\n",
    "stop_words=stop_list)\n",
    "\n",
    "#make train TFIDF\n",
    "temp = train_data.copy()\n",
    "train=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    train.append(remove_string_special_characters(x))\n",
    "\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train)\n",
    "\n",
    "#make test tfidf\n",
    "temp = test_data.copy()\n",
    "test=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    test.append(remove_string_special_characters(x))\n",
    "    \n",
    "tfidf_test = tfidf_vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need need to make Part Of Speech. So firstly we clean the text and find the tags for each word. Then for every sentence we find and save the percentage of the tags in each one. Lastly, we only keep the percentage of nouns, verds, adverbs and adjectives for every sentence and save them to an array. Then we concatenate the tfidf array with the Part of speech array and make them sparse again so we can use them in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train_data.copy()\n",
    "pos=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    pos.append(remove_string_special_characters(x)) #here we clean the text\n",
    "    \n",
    "\n",
    "diction=[]\n",
    "for x in pos:\n",
    "    tags = nltk.pos_tag(word_tokenize(x)) #here we make the tags for each word\n",
    "    counts = Counter(tag for word,tag in tags) # we count each tag\n",
    "    total=sum(counts.values())\n",
    "    #we put in a list a dictionary for each sentence which contains the tags and the percentage in the sentence\n",
    "    diction.append(dict((word, float(count)/total) for word,count in counts.items()))\n",
    "    \n",
    "T=[]\n",
    "\n",
    "y=0\n",
    "for x in diction: #for every dictionary in the list\n",
    "    noun=0\n",
    "    verb=0\n",
    "    adverb=0\n",
    "    adjective=0\n",
    "    # We make all the subclasses of noun one and if it exists in the sentence we add the percenatges\n",
    "    # We do this for verbs, adverbs and adjectives too\n",
    "    if 'NN' in x:\n",
    "        noun=noun+x['NN']\n",
    "    if 'NNS' in x:\n",
    "        noun=noun+x['NNS']\n",
    "    if 'NNP' in x:\n",
    "        noun=noun+x['NNP']\n",
    "    if 'NNPS' in x:\n",
    "        noun=noun+x['NNPS']\n",
    "    if 'VB' in x:\n",
    "        verb=verb+x['VB']\n",
    "    if 'VBD' in x:\n",
    "        verb=verb+x['VBD']\n",
    "    if 'VBG' in x:\n",
    "        verb=verb+x['VBG']\n",
    "    if 'VBN' in x:\n",
    "        verb=verb+x['VBN']\n",
    "    if 'VBP' in x:\n",
    "        verb=verb+x['VBP']\n",
    "    if 'VBZ' in x:\n",
    "        verb=verb+x['VBZ']\n",
    "    if 'RB' in x:\n",
    "        adverb=adverb+x['RB']\n",
    "    if 'RBR' in x:\n",
    "        adverb=adverb+x['RBR']\n",
    "    if 'RBS' in x:\n",
    "        adverb=adverb+x['RBS']\n",
    "    if 'JJ' in x:\n",
    "        adjective=adjective+x['JJ']\n",
    "    if 'JJR' in x:\n",
    "        adjective=adjective+x['JJR']\n",
    "    if 'JJS' in x:\n",
    "        adjective=adjective+x['JJS']\n",
    "\n",
    "    T.append([noun,verb,adverb,adjective])    \n",
    "#here we make the list a numoy array\n",
    "T=np.array(T)\n",
    "#To make the final array we concatenate tfidf array and the one that we made\n",
    "final_array=np.concatenate((tfidf_train.toarray(),T),axis=1)\n",
    "#And finally after creating the array we do them again sparse\n",
    "final_array = sparse.csr_matrix(final_array) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do exactly the same for the test_data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same method as above(train_data set)\n",
    "temp = test_data.copy()\n",
    "pos=[]\n",
    "for x in temp['Comment'].tolist():\n",
    "    pos.append(remove_string_special_characters(x))\n",
    "    \n",
    "\n",
    "diction1=[]\n",
    "for x in pos:\n",
    "    tags = nltk.pos_tag(word_tokenize(x))\n",
    "    counts = Counter(tag for word,tag in tags)\n",
    "    total=sum(counts.values())\n",
    "    diction1.append(dict((word, float(count)/total) for word,count in counts.items()))\n",
    "    \n",
    "T1=[]\n",
    "\n",
    "y=0\n",
    "for x in diction1:\n",
    "    noun=0\n",
    "    verb=0\n",
    "    adverb=0\n",
    "    adjective=0\n",
    "    if 'NN' in x:\n",
    "        noun=noun+x['NN']\n",
    "    if 'NNS' in x:\n",
    "        noun=noun+x['NNS']\n",
    "    if 'NNP' in x:\n",
    "        noun=noun+x['NNP']\n",
    "    if 'NNPS' in x:\n",
    "        noun=noun+x['NNPS']\n",
    "    if 'VB' in x:\n",
    "        verb=verb+x['VB']\n",
    "    if 'VBD' in x:\n",
    "        verb=verb+x['VBD']\n",
    "    if 'VBG' in x:\n",
    "        verb=verb+x['VBG']\n",
    "    if 'VBN' in x:\n",
    "        verb=verb+x['VBN']\n",
    "    if 'VBP' in x:\n",
    "        verb=verb+x['VBP']\n",
    "    if 'VBZ' in x:\n",
    "        verb=verb+x['VBZ']\n",
    "    if 'RB' in x:\n",
    "        adverb=adverb+x['RB']\n",
    "    if 'RBR' in x:\n",
    "        adverb=adverb+x['RBR']\n",
    "    if 'RBS' in x:\n",
    "        adverb=adverb+x['RBS']\n",
    "    if 'JJ' in x:\n",
    "        adjective=adjective+x['JJ']\n",
    "    if 'JJR' in x:\n",
    "        adjective=adjective+x['JJR']\n",
    "    if 'JJS' in x:\n",
    "        adjective=adjective+x['JJS']\n",
    "    T1.append([noun,verb,adverb,adjective])    \n",
    "    \n",
    "        \n",
    "T1=np.array(T1)\n",
    "\n",
    "final_test_array=np.concatenate((tfidf_test.toarray(),T1),axis=1)\n",
    "\n",
    "final_test_array = sparse.csr_matrix(final_test_array) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to use SVM and Random Decision Forest classifiers to see the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we try out the SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Support Vector Machines with tfidf-sof\n",
    "clf = svm.SVC()\n",
    "clf.fit(final_array, train_data['Insult'].array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.6536912751677852\n",
      "f1 score is: 0.4812332439678284\n"
     ]
    }
   ],
   "source": [
    "y_pred=clf.predict(final_test_array.toarray())\n",
    "print(\"Accuracy score is:\" ,metrics.accuracy_score(sol_test_data['Insult'].array, y_pred))\n",
    "print(\"f1 score is:\" ,metrics.f1_score(sol_test_data['Insult'].array, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are gonna test the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(final_array, train_data['Insult'].array);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.6644295302013423\n",
      "f1 score is: 0.5241116751269035\n"
     ]
    }
   ],
   "source": [
    "y_pred=rf.predict(final_test_array.toarray())\n",
    "print(\"Accuracy score is:\" ,metrics.accuracy_score(sol_test_data['Insult'].array, y_pred))\n",
    "print(\"f1 score is:\" ,metrics.f1_score(sol_test_data['Insult'].array, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results we can see that the way i have cleaned the data and the methods that i have used give us better results on the Random Forest Classifier after comparing the metrics. Acc score is about 1% better and f1 score is 4% better with the Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are gonna to try improve the results of one classifier! This one is GridSearch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase the results we will use GridSearch! GridSearch uses cross validation to find the best hyperparameters to use to train the classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] C=0.1, gamma=1, kernel=linear ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....... C=0.1, gamma=1, kernel=linear, score=0.743, total=   1.3s\n",
      "[CV] C=0.1, gamma=1, kernel=linear ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....... C=0.1, gamma=1, kernel=linear, score=0.744, total=   1.1s\n",
      "[CV] C=0.1, gamma=1, kernel=linear ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    2.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....... C=0.1, gamma=1, kernel=linear, score=0.744, total=   1.1s\n",
      "[CV] C=0.1, gamma=1, kernel=linear ...................................\n",
      "[CV] ....... C=0.1, gamma=1, kernel=linear, score=0.748, total=   1.1s\n",
      "[CV] C=0.1, gamma=1, kernel=linear ...................................\n",
      "[CV] ....... C=0.1, gamma=1, kernel=linear, score=0.752, total=   1.1s\n",
      "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n",
      "[CV] ..... C=0.1, gamma=0.1, kernel=linear, score=0.743, total=   1.1s\n",
      "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n",
      "[CV] ..... C=0.1, gamma=0.1, kernel=linear, score=0.744, total=   1.0s\n",
      "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n",
      "[CV] ..... C=0.1, gamma=0.1, kernel=linear, score=0.744, total=   1.1s\n",
      "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n",
      "[CV] ..... C=0.1, gamma=0.1, kernel=linear, score=0.748, total=   1.1s\n",
      "[CV] C=0.1, gamma=0.1, kernel=linear .................................\n",
      "[CV] ..... C=0.1, gamma=0.1, kernel=linear, score=0.752, total=   1.2s\n",
      "[CV] C=0.1, gamma=0.01, kernel=linear ................................\n",
      "[CV] .... C=0.1, gamma=0.01, kernel=linear, score=0.743, total=   1.1s\n",
      "[CV] C=0.1, gamma=0.01, kernel=linear ................................\n",
      "[CV] .... C=0.1, gamma=0.01, kernel=linear, score=0.744, total=   1.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=linear ................................\n",
      "[CV] .... C=0.1, gamma=0.01, kernel=linear, score=0.744, total=   1.1s\n",
      "[CV] C=0.1, gamma=0.01, kernel=linear ................................\n",
      "[CV] .... C=0.1, gamma=0.01, kernel=linear, score=0.748, total=   1.0s\n",
      "[CV] C=0.1, gamma=0.01, kernel=linear ................................\n",
      "[CV] .... C=0.1, gamma=0.01, kernel=linear, score=0.752, total=   1.1s\n",
      "[CV] C=0.1, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ... C=0.1, gamma=0.001, kernel=linear, score=0.743, total=   1.1s\n",
      "[CV] C=0.1, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ... C=0.1, gamma=0.001, kernel=linear, score=0.744, total=   1.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ... C=0.1, gamma=0.001, kernel=linear, score=0.744, total=   1.1s\n",
      "[CV] C=0.1, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ... C=0.1, gamma=0.001, kernel=linear, score=0.748, total=   1.0s\n",
      "[CV] C=0.1, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ... C=0.1, gamma=0.001, kernel=linear, score=0.752, total=   1.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] .. C=0.1, gamma=0.0001, kernel=linear, score=0.743, total=   1.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] .. C=0.1, gamma=0.0001, kernel=linear, score=0.744, total=   1.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] .. C=0.1, gamma=0.0001, kernel=linear, score=0.744, total=   1.0s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] .. C=0.1, gamma=0.0001, kernel=linear, score=0.748, total=   1.1s\n",
      "[CV] C=0.1, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] .. C=0.1, gamma=0.0001, kernel=linear, score=0.752, total=   1.2s\n",
      "[CV] C=1, gamma=1, kernel=linear .....................................\n",
      "[CV] ......... C=1, gamma=1, kernel=linear, score=0.839, total=   1.0s\n",
      "[CV] C=1, gamma=1, kernel=linear .....................................\n",
      "[CV] ......... C=1, gamma=1, kernel=linear, score=0.828, total=   1.0s\n",
      "[CV] C=1, gamma=1, kernel=linear .....................................\n",
      "[CV] ......... C=1, gamma=1, kernel=linear, score=0.828, total=   1.0s\n",
      "[CV] C=1, gamma=1, kernel=linear .....................................\n",
      "[CV] ......... C=1, gamma=1, kernel=linear, score=0.819, total=   1.0s\n",
      "[CV] C=1, gamma=1, kernel=linear .....................................\n",
      "[CV] ......... C=1, gamma=1, kernel=linear, score=0.821, total=   1.0s\n",
      "[CV] C=1, gamma=0.1, kernel=linear ...................................\n",
      "[CV] ....... C=1, gamma=0.1, kernel=linear, score=0.839, total=   1.0s\n",
      "[CV] C=1, gamma=0.1, kernel=linear ...................................\n",
      "[CV] ....... C=1, gamma=0.1, kernel=linear, score=0.828, total=   1.0s\n",
      "[CV] C=1, gamma=0.1, kernel=linear ...................................\n",
      "[CV] ....... C=1, gamma=0.1, kernel=linear, score=0.828, total=   1.0s\n",
      "[CV] C=1, gamma=0.1, kernel=linear ...................................\n",
      "[CV] ....... C=1, gamma=0.1, kernel=linear, score=0.819, total=   1.0s\n",
      "[CV] C=1, gamma=0.1, kernel=linear ...................................\n",
      "[CV] ....... C=1, gamma=0.1, kernel=linear, score=0.821, total=   1.0s\n",
      "[CV] C=1, gamma=0.01, kernel=linear ..................................\n",
      "[CV] ...... C=1, gamma=0.01, kernel=linear, score=0.839, total=   1.0s\n",
      "[CV] C=1, gamma=0.01, kernel=linear ..................................\n",
      "[CV] ...... C=1, gamma=0.01, kernel=linear, score=0.828, total=   1.0s\n",
      "[CV] C=1, gamma=0.01, kernel=linear ..................................\n",
      "[CV] ...... C=1, gamma=0.01, kernel=linear, score=0.828, total=   1.0s\n",
      "[CV] C=1, gamma=0.01, kernel=linear ..................................\n",
      "[CV] ...... C=1, gamma=0.01, kernel=linear, score=0.819, total=   1.0s\n",
      "[CV] C=1, gamma=0.01, kernel=linear ..................................\n",
      "[CV] ...... C=1, gamma=0.01, kernel=linear, score=0.821, total=   1.0s\n",
      "[CV] C=1, gamma=0.001, kernel=linear .................................\n",
      "[CV] ..... C=1, gamma=0.001, kernel=linear, score=0.839, total=   1.1s\n",
      "[CV] C=1, gamma=0.001, kernel=linear .................................\n",
      "[CV] ..... C=1, gamma=0.001, kernel=linear, score=0.828, total=   1.0s\n",
      "[CV] C=1, gamma=0.001, kernel=linear .................................\n",
      "[CV] ..... C=1, gamma=0.001, kernel=linear, score=0.828, total=   1.0s\n",
      "[CV] C=1, gamma=0.001, kernel=linear .................................\n",
      "[CV] ..... C=1, gamma=0.001, kernel=linear, score=0.819, total=   1.0s\n",
      "[CV] C=1, gamma=0.001, kernel=linear .................................\n",
      "[CV] ..... C=1, gamma=0.001, kernel=linear, score=0.821, total=   1.1s\n",
      "[CV] C=1, gamma=0.0001, kernel=linear ................................\n",
      "[CV] .... C=1, gamma=0.0001, kernel=linear, score=0.839, total=   1.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=linear ................................\n",
      "[CV] .... C=1, gamma=0.0001, kernel=linear, score=0.828, total=   1.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=linear ................................\n",
      "[CV] .... C=1, gamma=0.0001, kernel=linear, score=0.828, total=   1.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=linear ................................\n",
      "[CV] .... C=1, gamma=0.0001, kernel=linear, score=0.819, total=   1.0s\n",
      "[CV] C=1, gamma=0.0001, kernel=linear ................................\n",
      "[CV] .... C=1, gamma=0.0001, kernel=linear, score=0.821, total=   1.0s\n",
      "[CV] C=10, gamma=1, kernel=linear ....................................\n",
      "[CV] ........ C=10, gamma=1, kernel=linear, score=0.784, total=   1.4s\n",
      "[CV] C=10, gamma=1, kernel=linear ....................................\n",
      "[CV] ........ C=10, gamma=1, kernel=linear, score=0.773, total=   1.3s\n",
      "[CV] C=10, gamma=1, kernel=linear ....................................\n",
      "[CV] ........ C=10, gamma=1, kernel=linear, score=0.786, total=   1.4s\n",
      "[CV] C=10, gamma=1, kernel=linear ....................................\n",
      "[CV] ........ C=10, gamma=1, kernel=linear, score=0.796, total=   1.4s\n",
      "[CV] C=10, gamma=1, kernel=linear ....................................\n",
      "[CV] ........ C=10, gamma=1, kernel=linear, score=0.767, total=   1.5s\n",
      "[CV] C=10, gamma=0.1, kernel=linear ..................................\n",
      "[CV] ...... C=10, gamma=0.1, kernel=linear, score=0.784, total=   1.4s\n",
      "[CV] C=10, gamma=0.1, kernel=linear ..................................\n",
      "[CV] ...... C=10, gamma=0.1, kernel=linear, score=0.773, total=   1.5s\n",
      "[CV] C=10, gamma=0.1, kernel=linear ..................................\n",
      "[CV] ...... C=10, gamma=0.1, kernel=linear, score=0.786, total=   1.4s\n",
      "[CV] C=10, gamma=0.1, kernel=linear ..................................\n",
      "[CV] ...... C=10, gamma=0.1, kernel=linear, score=0.796, total=   1.4s\n",
      "[CV] C=10, gamma=0.1, kernel=linear ..................................\n",
      "[CV] ...... C=10, gamma=0.1, kernel=linear, score=0.767, total=   1.4s\n",
      "[CV] C=10, gamma=0.01, kernel=linear .................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... C=10, gamma=0.01, kernel=linear, score=0.784, total=   1.4s\n",
      "[CV] C=10, gamma=0.01, kernel=linear .................................\n",
      "[CV] ..... C=10, gamma=0.01, kernel=linear, score=0.773, total=   1.3s\n",
      "[CV] C=10, gamma=0.01, kernel=linear .................................\n",
      "[CV] ..... C=10, gamma=0.01, kernel=linear, score=0.786, total=   1.4s\n",
      "[CV] C=10, gamma=0.01, kernel=linear .................................\n",
      "[CV] ..... C=10, gamma=0.01, kernel=linear, score=0.796, total=   1.4s\n",
      "[CV] C=10, gamma=0.01, kernel=linear .................................\n",
      "[CV] ..... C=10, gamma=0.01, kernel=linear, score=0.767, total=   1.4s\n",
      "[CV] C=10, gamma=0.001, kernel=linear ................................\n",
      "[CV] .... C=10, gamma=0.001, kernel=linear, score=0.784, total=   1.5s\n",
      "[CV] C=10, gamma=0.001, kernel=linear ................................\n",
      "[CV] .... C=10, gamma=0.001, kernel=linear, score=0.773, total=   1.4s\n",
      "[CV] C=10, gamma=0.001, kernel=linear ................................\n",
      "[CV] .... C=10, gamma=0.001, kernel=linear, score=0.786, total=   1.4s\n",
      "[CV] C=10, gamma=0.001, kernel=linear ................................\n",
      "[CV] .... C=10, gamma=0.001, kernel=linear, score=0.796, total=   1.4s\n",
      "[CV] C=10, gamma=0.001, kernel=linear ................................\n",
      "[CV] .... C=10, gamma=0.001, kernel=linear, score=0.767, total=   1.4s\n",
      "[CV] C=10, gamma=0.0001, kernel=linear ...............................\n",
      "[CV] ... C=10, gamma=0.0001, kernel=linear, score=0.784, total=   1.5s\n",
      "[CV] C=10, gamma=0.0001, kernel=linear ...............................\n",
      "[CV] ... C=10, gamma=0.0001, kernel=linear, score=0.773, total=   1.4s\n",
      "[CV] C=10, gamma=0.0001, kernel=linear ...............................\n",
      "[CV] ... C=10, gamma=0.0001, kernel=linear, score=0.786, total=   1.4s\n",
      "[CV] C=10, gamma=0.0001, kernel=linear ...............................\n",
      "[CV] ... C=10, gamma=0.0001, kernel=linear, score=0.796, total=   1.4s\n",
      "[CV] C=10, gamma=0.0001, kernel=linear ...............................\n",
      "[CV] ... C=10, gamma=0.0001, kernel=linear, score=0.767, total=   1.4s\n",
      "[CV] C=100, gamma=1, kernel=linear ...................................\n",
      "[CV] ....... C=100, gamma=1, kernel=linear, score=0.763, total=   1.9s\n",
      "[CV] C=100, gamma=1, kernel=linear ...................................\n",
      "[CV] ....... C=100, gamma=1, kernel=linear, score=0.756, total=   1.9s\n",
      "[CV] C=100, gamma=1, kernel=linear ...................................\n",
      "[CV] ....... C=100, gamma=1, kernel=linear, score=0.766, total=   2.1s\n",
      "[CV] C=100, gamma=1, kernel=linear ...................................\n",
      "[CV] ....... C=100, gamma=1, kernel=linear, score=0.750, total=   2.4s\n",
      "[CV] C=100, gamma=1, kernel=linear ...................................\n",
      "[CV] ....... C=100, gamma=1, kernel=linear, score=0.752, total=   2.3s\n",
      "[CV] C=100, gamma=0.1, kernel=linear .................................\n",
      "[CV] ..... C=100, gamma=0.1, kernel=linear, score=0.763, total=   2.0s\n",
      "[CV] C=100, gamma=0.1, kernel=linear .................................\n",
      "[CV] ..... C=100, gamma=0.1, kernel=linear, score=0.756, total=   2.2s\n",
      "[CV] C=100, gamma=0.1, kernel=linear .................................\n",
      "[CV] ..... C=100, gamma=0.1, kernel=linear, score=0.766, total=   2.1s\n",
      "[CV] C=100, gamma=0.1, kernel=linear .................................\n",
      "[CV] ..... C=100, gamma=0.1, kernel=linear, score=0.750, total=   2.5s\n",
      "[CV] C=100, gamma=0.1, kernel=linear .................................\n",
      "[CV] ..... C=100, gamma=0.1, kernel=linear, score=0.752, total=   1.9s\n",
      "[CV] C=100, gamma=0.01, kernel=linear ................................\n",
      "[CV] .... C=100, gamma=0.01, kernel=linear, score=0.763, total=   1.9s\n",
      "[CV] C=100, gamma=0.01, kernel=linear ................................\n",
      "[CV] .... C=100, gamma=0.01, kernel=linear, score=0.756, total=   2.1s\n",
      "[CV] C=100, gamma=0.01, kernel=linear ................................\n",
      "[CV] .... C=100, gamma=0.01, kernel=linear, score=0.766, total=   2.1s\n",
      "[CV] C=100, gamma=0.01, kernel=linear ................................\n",
      "[CV] .... C=100, gamma=0.01, kernel=linear, score=0.750, total=   2.6s\n",
      "[CV] C=100, gamma=0.01, kernel=linear ................................\n",
      "[CV] .... C=100, gamma=0.01, kernel=linear, score=0.752, total=   2.6s\n",
      "[CV] C=100, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ... C=100, gamma=0.001, kernel=linear, score=0.763, total=   2.5s\n",
      "[CV] C=100, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ... C=100, gamma=0.001, kernel=linear, score=0.756, total=   1.9s\n",
      "[CV] C=100, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ... C=100, gamma=0.001, kernel=linear, score=0.766, total=   2.3s\n",
      "[CV] C=100, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ... C=100, gamma=0.001, kernel=linear, score=0.750, total=   2.7s\n",
      "[CV] C=100, gamma=0.001, kernel=linear ...............................\n",
      "[CV] ... C=100, gamma=0.001, kernel=linear, score=0.752, total=   1.9s\n",
      "[CV] C=100, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] .. C=100, gamma=0.0001, kernel=linear, score=0.763, total=   1.9s\n",
      "[CV] C=100, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] .. C=100, gamma=0.0001, kernel=linear, score=0.756, total=   1.8s\n",
      "[CV] C=100, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] .. C=100, gamma=0.0001, kernel=linear, score=0.766, total=   2.1s\n",
      "[CV] C=100, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] .. C=100, gamma=0.0001, kernel=linear, score=0.750, total=   2.5s\n",
      "[CV] C=100, gamma=0.0001, kernel=linear ..............................\n",
      "[CV] .. C=100, gamma=0.0001, kernel=linear, score=0.752, total=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=1,\n",
       "             param_grid={'C': [0.1, 1, 10, 100],\n",
       "                         'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                         'kernel': ['linear']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Support Vector Machines with tfidf\n",
    "clf = svm.SVC()\n",
    "param_grid = {'C': [0.1, 1, 10, 100],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['linear']}  \n",
    "grid = GridSearchCV(clf, param_grid, refit = True, verbose = 3,cv=StratifiedKFold(n_splits=5),n_jobs=1)\n",
    "grid.fit(final_array, train_data['Insult'].array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results after the GridSearch arethe below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.6774049217002237\n",
      "f1 score is: 0.5530068195908245\n"
     ]
    }
   ],
   "source": [
    "y_pred=grid.predict(final_test_array.toarray())\n",
    "print(\"Accuracy score is:\" ,metrics.accuracy_score(sol_test_data['Insult'].array, y_pred))\n",
    "print(\"f1 score is:\" ,metrics.f1_score(sol_test_data['Insult'].array, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the results are better now! Accuracy is **bigger for about 2%** and f1 metric **has increased 7%** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
